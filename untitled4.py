# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e08-NIO9VqjZPvZ7RxgIYaKbS7IRYLeO
"""

import matplotlib.pyplot as plt
from matplotlib.patches import FancyBboxPatch
from matplotlib.patches import ArrowStyle

# Initialize the figure
fig, ax = plt.subplots(figsize=(12, 8))

# Function to draw a box with text
def draw_box(ax, text, x, y, width=0.2, height=0.1):
    bbox = FancyBboxPatch((x, y), width, height, boxstyle="round,pad=0.1", edgecolor="black", facecolor="#a8dadc")
    ax.add_patch(bbox)
    ax.text(x + width / 2, y + height / 2, text, ha="center", va="center", fontsize=10)

# Draw boxes representing the steps
draw_box(ax, "Data Collection", 0.1, 0.8)
draw_box(ax, "Data Preprocessing", 0.4, 0.8)
draw_box(ax, "Feature Extraction", 0.7, 0.8)
draw_box(ax, "Classification Model", 0.4, 0.6)
draw_box(ax, "Real-Time Detection", 0.4, 0.4)
draw_box(ax, "User Interface and Alerts", 0.1, 0.4)
draw_box(ax, "Feedback Loop", 0.1, 0.2)
draw_box(ax, "Integration with Social Media", 0.7, 0.4)

# Draw arrows between the boxes
arrowprops = dict(arrowstyle=ArrowStyle("-|>", head_length=0.4, head_width=0.2), color="black")

ax.annotate('', xy=(0.35, 0.85), xytext=(0.28, 0.85), arrowprops=arrowprops) # Data Collection to Data Preprocessing
ax.annotate('', xy=(0.65, 0.85), xytext=(0.58, 0.85), arrowprops=arrowprops) # Data Preprocessing to Feature Extraction
ax.annotate('', xy=(0.5, 0.75), xytext=(0.5, 0.7), arrowprops=arrowprops)    # Feature Extraction to Classification Model
ax.annotate('', xy=(0.5, 0.55), xytext=(0.5, 0.5), arrowprops=arrowprops)    # Classification Model to Real-Time Detection
ax.annotate('', xy=(0.3, 0.45), xytext=(0.28, 0.45), arrowprops=arrowprops)  # Real-Time Detection to User Interface and Alerts
ax.annotate('', xy=(0.2, 0.35), xytext=(0.2, 0.25), arrowprops=arrowprops)   # User Interface and Alerts to Feedback Loop
ax.annotate('', xy=(0.65, 0.45), xytext=(0.58, 0.45), arrowprops=arrowprops) # Real-Time Detection to Integration with Social Media

# Remove axes
ax.axis('off')

# Set title
plt.title("Flowchart of AI Model for Fake News Detection", fontsize=16)
plt.show()

# Initialize the figure again
fig, ax = plt.subplots(figsize=(14, 10))

# Function to draw a box with text
def draw_box(ax, text, x, y, width=0.3, height=0.1, color="#f1faee"):
    bbox = FancyBboxPatch((x, y), width, height, boxstyle="round,pad=0.1", edgecolor="black", facecolor=color)
    ax.add_patch(bbox)
    ax.text(x + width / 2, y + height / 2, text, ha="center", va="center", fontsize=12, fontweight='bold')

# Draw the boxes for each component
draw_box(ax, "Data Ingestion", 0.1, 0.7, color="#a8dadc")
draw_box(ax, "Model Training", 0.4, 0.7, color="#457b9d")
draw_box(ax, "API for Deployment", 0.4, 0.5, color="#1d3557")
draw_box(ax, "User Interface", 0.7, 0.6, color="#e63946")

# Draw the central data storage box
draw_box(ax, "Central Repository", 0.1, 0.5, color="#f4a261")

# Draw arrows to indicate data flow
ax.annotate('', xy=(0.25, 0.65), xytext=(0.25, 0.55), arrowprops=dict(facecolor='black', shrink=0.05))
ax.annotate('', xy=(0.4, 0.65), xytext=(0.25, 0.55), arrowprops=dict(facecolor='black', shrink=0.05))
ax.annotate('', xy=(0.25, 0.55), xytext=(0.4, 0.65), arrowprops=dict(facecolor='black', shrink=0.05))
ax.annotate('', xy=(0.55, 0.65), xytext=(0.55, 0.55), arrowprops=dict(facecolor='black', shrink=0.05))
ax.annotate('', xy=(0.55, 0.55), xytext=(0.7, 0.65), arrowprops=dict(facecolor='black', shrink=0.05))

# Remove axes
ax.axis('off')

# Set title
plt.title("High-Level Technology Architecture for Fake News Detection System", fontsize=16)
plt.show()

pip install pandas numpy scikit-learn nltk

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

import pandas as pd

# Try to read the file, handling potential errors
try:
    df = pd.read_csv('/content/news_articles.csv')
    print(df.head())
except pd.errors.ParserError as e:
    print(f"Error reading CSV: {e}")
    # If the error persists, try opening the file in a text editor
    # and inspect the lines around row 1283 to look for unterminated strings.

nltk.download('stopwords')
nltk.download('punkt')

# Function to clean text
def clean_text(text):
    # Tokenization
    tokens = word_tokenize(text)
    # Lowercasing
    tokens = [word.lower() for word in tokens]
    # Removing punctuation
    table = str.maketrans('', '', string.punctuation)
    tokens = [word.translate(table) for word in tokens]
    # Removing stopwords
    tokens = [word for word in tokens if word not in stopwords.words('english')]
    return ' '.join(tokens)

# Apply cleaning
df['text'] = df['text'].apply(clean_text)

X = df['text']
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert text to TF-IDF features
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

model = LogisticRegression()
model.fit(X_train_tfidf, y_train)

# Predict on test data
y_pred = model.predict(X_test_tfidf)

# Evaluate the model
print(f'Accuracy: {accuracy_score(y_test, y_pred)}')
print(classification_report(y_test, y_pred))

# Example prediction
new_text = ["The government has confirmed the new policy on healthcare."]
new_text_cleaned = [clean_text(text) for text in new_text]
new_text_tfidf = vectorizer.transform(new_text_cleaned)
prediction = model.predict(new_text_tfidf)
print(f'Prediction: {prediction[0]}')  # 'fake' or 'real'





